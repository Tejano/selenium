
To reduce setup even further and make this solution more generic, you can use dynamic schema detection to automatically retrieve data types and constraints directly from SQL Server. This way, you won’t need to manually specify field names, data types, or constraints for each new case.

Here’s how to achieve an even more automated solution:

Enhanced Python Solution with Dynamic Schema Detection
This approach dynamically pulls metadata from SQL Server to identify numeric columns and their constraints, making it more flexible for various tables without requiring manual setup for each field.

Steps:
Auto-Detect Column Types and Constraints: Retrieve the table’s schema information directly from SQL Server.
Apply Validation Based on Detected Constraints: Use this metadata to validate data for numeric overflow without hardcoding field constraints.
Log Issues Automatically: Output only problematic records, highlighting the specific field and value causing the overflow.





import pyodbc
import pandas as pd

# SQL Server connection setup
connection_string = 'DRIVER={SQL Server};SERVER=your_server;DATABASE=your_db;UID=your_user;PWD=your_password'
conn = pyodbc.connect(connection_string)

# Define the table you want to check
table_name = "YourSourceTable"

# Step 1: Retrieve column metadata (name, type, precision, scale) for numeric fields
metadata_query = f"""
SELECT COLUMN_NAME, DATA_TYPE, NUMERIC_PRECISION, NUMERIC_SCALE
FROM INFORMATION_SCHEMA.COLUMNS
WHERE TABLE_NAME = '{table_name}' AND DATA_TYPE IN ('numeric', 'decimal', 'float', 'real')
"""
metadata = pd.read_sql(metadata_query, conn)

# Step 2: Retrieve the table data
data_query = f"SELECT * FROM {table_name}"
data = pd.read_sql(data_query, conn)

# Step 3: Validate each numeric field based on its metadata constraints
def validate_numeric(row, column, precision, scale):
    try:
        max_value = 10 ** (precision - scale) - 10 ** -scale
        min_value = -max_value
        value = float(row[column])
        if not (min_value <= value <= max_value):
            return f"Overflow in {column}: {value}"
    except:
        return f"Invalid data in {column}: {row[column]}"
    return None

# Step 4: Apply validation for each numeric column and log issues
issues = []
for idx, row in data.iterrows():
    for _, col_meta in metadata.iterrows():
        column = col_meta['COLUMN_NAME']
        precision = col_meta['NUMERIC_PRECISION']
        scale = col_meta['NUMERIC_SCALE']
        issue = validate_numeric(row, column, precision, scale)
        if issue:
            issues.append({"Row": idx, "Issue": issue})

# Step 5: Output the issues
if issues:
    issues_df = pd.DataFrame(issues)
    print(issues_df)
    issues_df.to_csv("validation_issues.csv", index=False)  # Optional: save to CSV for recordkeeping
else:
    print("No issues found.")
	
How This Solution Reduces Manual Setup:
Automated Schema Detection: The script fetches numeric column constraints (NUMERIC_PRECISION and NUMERIC_SCALE) directly from the database, eliminating the need to define them manually.
Universal Validation Function: This function applies universally to all numeric fields without any need for modification.
Flexible Output: The results can be viewed directly in the console or saved to a CSV file with no extra configuration.
Advantages:
Reusable: You only need to specify the table name, and the component handles the rest.
Minimal Manual Intervention: Dynamically retrieves schema information, so no need to hard-code column names or types.
Easy Logging: Generates a CSV report of problematic fields and values, which can be saved or reviewed directly.
This solution automates as much as possible, making it adaptable to different tables and schemas with almost no setup for each new case.

*** With File
Yes, you can absolutely provide the INSERT...SELECT SQL snippet as a file, and the script can parse this file to dynamically extract the relevant source and target columns. This approach minimizes manual setup by automating the column mapping based on the contents of the SQL file.

Here's how the process would work:

Read the SQL File: The script reads the SQL snippet containing the INSERT...SELECT statement.
Parse the Statement: It extracts the list of source and target columns from the SQL snippet.
Use the Parsed Columns for Validation: With the extracted columns, the script then validates the source data against the constraints of the target table.
Python Script to Parse the INSERT...SELECT Statement from a File
This script will:

Read and parse an INSERT...SELECT SQL file.
Extract source and target column mappings.
Use those mappings to check for overflow issues based on target table constraints.
import pyodbc
import pandas as pd
import re

# SQL Server connection setup
connection_string = 'DRIVER={SQL Server};SERVER=your_server;DATABASE=your_db;UID=your_user;PWD=your_password'
conn = pyodbc.connect(connection_string)

# Path to the SQL file containing the INSERT...SELECT statement
sql_file_path = "insert_select.sql"

# Step 1: Read and parse the SQL file to extract source and target columns
with open(sql_file_path, 'r') as file:
    sql_content = file.read()

# Regex to extract target table, target columns, source table, and source columns
insert_pattern = re.search(r"INSERT INTO (\w+)\s*\(([^)]+)\)", sql_content, re.IGNORECASE)
select_pattern = re.search(r"SELECT (.+?) FROM (\w+)", sql_content, re.IGNORECASE)

# Extract target and source tables and columns
target_table = insert_pattern.group(1).strip()
target_columns = [col.strip() for col in insert_pattern.group(2).split(",")]
source_table = select_pattern.group(2).strip()
source_columns = [col.strip() for col in select_pattern.group(1).split(",")]

# Step 2: Create a dictionary to map source columns to target columns
column_mapping = dict(zip(source_columns, target_columns))

# Step 3: Retrieve metadata for the target columns based on the mapping
target_columns_str = "', '".join(target_columns)
metadata_query = f"""
SELECT COLUMN_NAME, DATA_TYPE, NUMERIC_PRECISION, NUMERIC_SCALE
FROM INFORMATION_SCHEMA.COLUMNS
WHERE TABLE_NAME = '{target_table}' 
  AND COLUMN_NAME IN ('{target_columns_str}')
  AND DATA_TYPE IN ('numeric', 'decimal', 'float', 'real')
"""
metadata = pd.read_sql(metadata_query, conn)

# Step 4: Retrieve data from the source table (only the relevant columns)
source_columns_str = ', '.join(source_columns)
data_query = f"SELECT {source_columns_str} FROM {source_table}"
data = pd.read_sql(data_query, conn)

# Step 5: Validate each source column based on target constraints
def validate_numeric(row, source_column, precision, scale):
    try:
        max_value = 10 ** (precision - scale) - 10 ** -scale
        min_value = -max_value
        value = float(row[source_column])
        if not (min_value <= value <= max_value):
            return f"Overflow in {source_column} with value {value}"
    except:
        return f"Invalid data in {source_column} with value {row[source_column]}"
    return None

# Step 6: Apply validation for each column based on the target's constraints
issues = []
for idx, row in data.iterrows():
    for _, col_meta in metadata.iterrows():
        target_column = col_meta['COLUMN_NAME']
        precision = col_meta['NUMERIC_PRECISION']
        scale = col_meta['NUMERIC_SCALE']
        
        # Find corresponding source column based on mapping
        source_column = next((k for k, v in column_mapping.items() if v == target_column), None)
        if source_column:
            issue = validate_numeric(row, source_column, precision, scale)
            if issue:
                issues.append({"Row": idx, "Issue": issue})

# Step 7: Output the issues
if issues:
    issues_df = pd.DataFrame(issues)
    print(issues_df)
    issues_df.to_csv("validation_issues.csv", index=False)  # Optional: save to CSV for recordkeeping
else:
    print("No issues found.")

Explanation of the Script
SQL File Parsing:

The script reads the SQL file and uses regular expressions to extract:
The target table and target columns.
The source table and source columns.
Dynamic Column Mapping:

A dictionary (column_mapping) is created to map source columns to target columns automatically based on the INSERT...SELECT statement.
Retrieve Target Metadata:

Using the target columns from the mapping, the script fetches the constraints (precision and scale) for each numeric target column.
Source Data Retrieval:

Only the relevant source columns are selected from the source table.
Validation:

Each source column’s values are validated against the target table constraints. Any issues (overflow or invalid data) are logged.
Output:

If issues are found, they’re printed and saved to a CSV file for review.
Benefits of This Approach
Fully Automated Column Mapping: The script automatically extracts column mappings from the SQL file, eliminating the need for manual setup.
Reusable for Various INSERT...SELECT Statements: You can swap out the SQL file without modifying the script.
Simplified Setup: Just provide the INSERT...SELECT SQL file path, and the script handles the rest.
This approach gives you a versatile tool for automated validation based on the actual SQL statement, saving time and minimizing setup requirements. Let me know if you need any additional adjustments!
]

import pyodbc

connection_string = (
    "DRIVER={ODBC Driver 17 for SQL Server};"
    "SERVER=your_server_name;"
    "DATABASE=your_database_name;"
    "UID=your_username;"
    "PWD=your_password;"
)
conn = pyodbc.connect(connection_string)

import pyodbc

connection_string = (
    "DRIVER={ODBC Driver 17 for SQL Server};"
    "SERVER=your_server_name;"
    "DATABASE=your_database_name;"
    "Trusted_Connection=yes;"
)
conn = pyodbc.connect(connection_string)


import pyodbc
import pandas as pd
import re

# Step 1: Specify the file path to the SQL file on the C: drive
sql_file_path = r"C:\path\to\your\insert_select.sql"

# Step 2: Read the SQL file
with open(sql_file_path, 'r') as file:
    sql_content = file.read()
	sql_content_cleaned = re.sub(r"(--.*?\n|DECLARE .*?;|SET .*?;)", "", sql_content, flags=re.IGNORECASE | re.DOTALL)
print("SQL Content Read from File:")
print(sql_content)
# Step 3: Parse the SQL content to extract source and target tables and columns
#insert_pattern = re.search(r"INSERT INTO (\w+)\s*\(([^)]+)\)", sql_content, re.IGNORECASE)


# Adjust the regular expression to find the first INSERT INTO statement

insert_pattern = re.search(
 r"INSERT INTO\s+(\w+)\s*\(([^)]+)\)",
 sql_content_cleaned, re.IGNORECASE | re.DOTALL)
if not insert_pattern:
    raise ValueError("The SQL file does not contain a valid INSERT INTO statement.")

# Extract target and source tables and columns
target_table = insert_pattern.group(1).strip()
target_columns = [col.strip() for col in insert_pattern.group(2).split(",")]

# Debugging: Print extracted information
print(f"Target Table: {target_table}")
print(f"Target Columns: {target_columns}")

select_pattern = re.search(r"SELECT (.+?) FROM (\w+)", sql_content_cleaned, re.IGNORECASE)
source_table = select_pattern.group(2).strip()
source_columns = [col.strip() for col in select_pattern.group(1).split(",")]

# Step 4: Map source columns to target columns
column_mapping = dict(zip(source_columns, target_columns))

# Step 5: Set up the connection string for Windows Authentication
connection_string = (
    "DRIVER={ODBC Driver 17 for SQL Server};"
    "SERVER=your_server_name;"
    "DATABASE=your_database_name;"
    "Trusted_Connection=yes;"
)

try:
    # Connect to the database
    conn = pyodbc.connect(connection_string)
    cursor = conn.cursor()

    # Step 6: Retrieve metadata for the target columns
    target_columns_str = "', '".join(target_columns)
    metadata_query = f"""
    SELECT COLUMN_NAME, DATA_TYPE, NUMERIC_PRECISION, NUMERIC_SCALE
    FROM INFORMATION_SCHEMA.COLUMNS
    WHERE TABLE_NAME = '{target_table}' 
      AND COLUMN_NAME IN ('{target_columns_str}')
      AND DATA_TYPE IN ('numeric', 'decimal', 'float', 'real')
    """
    metadata = pd.read_sql(metadata_query, conn)

    # Step 7: Retrieve data from the source table
    source_columns_str = ', '.join(source_columns)
    source_query = f"SELECT {source_columns_str} FROM {source_table}"
    source_data = pd.read_sql(source_query, conn)

    # Step 8: Validate each numeric field in the source table against the target constraints
    def validate_numeric(row, source_column, precision, scale):
        try:
            max_value = 10 ** (precision - scale) - 10 ** -scale
            min_value = -max_value
            value = float(row[source_column])
            if not (min_value <= value <= max_value):
                return f"Overflow in {source_column} with value {value}"
        except:
            return f"Invalid data in {source_column} with value {row[source_column]}"
        return None

    # Apply validation and collect issues
    issues = []
    for idx, row in source_data.iterrows():
        for _, col_meta in metadata.iterrows():
            target_column = col_meta['COLUMN_NAME']
            precision = col_meta['NUMERIC_PRECISION']
            scale = col_meta['NUMERIC_SCALE']
            
            # Map the target column back to the source column
            source_column = next((k for k, v in column_mapping.items() if v == target_column), None)
            if source_column:
                issue = validate_numeric(row, source_column, precision, scale)
                if issue:
                    issues.append({"Row": idx, "Issue": issue})

    # Step 9: Output the issues
    if issues:
        issues_df = pd.DataFrame(issues)
        print(issues_df)
        issues_df.to_csv("validation_issues.csv", index=False)  # Optional: save to CSV
    else:
        print("No overflow issues found.")

except pyodbc.Error as e:
    print("Database error occurred:", e)

finally:
    # Close the connection
    if 'cursor' in locals():
        cursor.close()
    if 'conn' in locals():
        conn.close()


# Read the SQL file
with open(sql_file_path, 'r') as file:
    sql_content = file.read()

# Debugging: Print the SQL content to confirm it's read correctly
print("SQL Content:")
print(sql_content)

# Adjust the regular expression to find the INSERT INTO statement
insert_pattern = re.search(
    r"INSERT INTO\s+(\w+)\s*\((.*?)\)\s*SELECT",
    sql_content,
    re.IGNORECASE | re.DOTALL
)

if not insert_pattern:
    print("Debug: Unable to match the INSERT INTO statement. Check the SQL content:")
    print(sql_content)
    raise ValueError("The SQL file does not contain a valid INSERT INTO statement.")

# Extract target table and columns
target_table = insert_pattern.group(1).strip()
target_columns = [col.strip() for col in insert_pattern.group(2).split(",")]

# Debugging: Print extracted information
print(f"Target Table: {target_table}")
print(f"Target Columns: {target_columns}")



//////////////////////////////////////////////////////////

# Read the SQL file
with open(sql_file_path, 'r') as file:
    sql_content = file.read()

# Debugging: Print the SQL content to confirm it's read correctly
print("SQL Content:")
print(sql_content)

# Adjust the regular expression to find the INSERT INTO statement
insert_pattern = re.search(
    r"INSERT INTO\s+(\w+)\s*\((.*?)\)\s*SELECT",
    sql_content,
    re.IGNORECASE | re.DOTALL
)

if not insert_pattern:
    print("Debug: Unable to match the INSERT INTO statement. Check the SQL content:")
    print(sql_content)
    raise ValueError("The SQL file does not contain a valid INSERT INTO statement.")

# Extract target table and columns
target_table = insert_pattern.group(1).strip()
target_columns = [col.strip() for col in insert_pattern.group(2).split(",")]

# Debugging: Print extracted information
print(f"Target Table: {target_table}")
print(f"Target Columns: {target_columns}")

